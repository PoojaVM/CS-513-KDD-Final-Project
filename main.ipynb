{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course - CS-513 Knowledge Discovery and Data Mining\n",
    "#### Problem Statement - Predict the prices of real estate in New York City using the dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84548 entries, 0 to 84547\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   Unnamed: 0                      84548 non-null  int64 \n",
      " 1   BOROUGH                         84548 non-null  int64 \n",
      " 2   NEIGHBORHOOD                    84548 non-null  object\n",
      " 3   BUILDING CLASS CATEGORY         84548 non-null  object\n",
      " 4   TAX CLASS AT PRESENT            84548 non-null  object\n",
      " 5   BLOCK                           84548 non-null  int64 \n",
      " 6   LOT                             84548 non-null  int64 \n",
      " 7   EASE-MENT                       84548 non-null  object\n",
      " 8   BUILDING CLASS AT PRESENT       84548 non-null  object\n",
      " 9   ADDRESS                         84548 non-null  object\n",
      " 10  APARTMENT NUMBER                84548 non-null  object\n",
      " 11  ZIP CODE                        84548 non-null  int64 \n",
      " 12  RESIDENTIAL UNITS               84548 non-null  int64 \n",
      " 13  COMMERCIAL UNITS                84548 non-null  int64 \n",
      " 14  TOTAL UNITS                     84548 non-null  int64 \n",
      " 15  LAND SQUARE FEET                84548 non-null  object\n",
      " 16  GROSS SQUARE FEET               84548 non-null  object\n",
      " 17  YEAR BUILT                      84548 non-null  int64 \n",
      " 18  TAX CLASS AT TIME OF SALE       84548 non-null  int64 \n",
      " 19  BUILDING CLASS AT TIME OF SALE  84548 non-null  object\n",
      " 20  SALE PRICE                      84548 non-null  object\n",
      " 21  SALE DATE                       84548 non-null  object\n",
      "dtypes: int64(10), object(12)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('nyc-rolling-sales.csv')\n",
    "\n",
    "# Dataset columns pre-cleanup\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84548 entries, 0 to 84547\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   BOROUGH                         84548 non-null  int64 \n",
      " 1   NEIGHBORHOOD                    84548 non-null  object\n",
      " 2   BUILDING CLASS CATEGORY         84548 non-null  object\n",
      " 3   TAX CLASS AT PRESENT            83810 non-null  object\n",
      " 4   BLOCK                           84548 non-null  int64 \n",
      " 5   BUILDING CLASS AT PRESENT       83810 non-null  object\n",
      " 6   APARTMENT NUMBER                19052 non-null  object\n",
      " 7   ZIP CODE                        84548 non-null  int64 \n",
      " 8   RESIDENTIAL UNITS               84548 non-null  int64 \n",
      " 9   COMMERCIAL UNITS                84548 non-null  int64 \n",
      " 10  TOTAL UNITS                     84548 non-null  int64 \n",
      " 11  LAND SQUARE FEET                58296 non-null  object\n",
      " 12  GROSS SQUARE FEET               56936 non-null  object\n",
      " 13  YEAR BUILT                      84548 non-null  int64 \n",
      " 14  TAX CLASS AT TIME OF SALE       84548 non-null  int64 \n",
      " 15  BUILDING CLASS AT TIME OF SALE  84548 non-null  object\n",
      " 16  SALE PRICE                      69987 non-null  object\n",
      "dtypes: int64(8), object(9)\n",
      "memory usage: 11.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Clean dataset\n",
    "\n",
    "# Drop Unnamed: 0 feature which is just a serial number\n",
    "# Drop ADDRESS and LOT features as it is not required for analysis.\n",
    "df.drop([\"ADDRESS\", \"Unnamed: 0\", \"LOT\", \"SALE DATE\"], axis=1, inplace=True)\n",
    "\n",
    "# Check and drop columns where all cells are empty or -\n",
    "df = df.applymap(lambda x: pd.NA if str(x).strip() in ['-', ''] else x)\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "# Dataset columns post-cleanup\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Rows, Columns): (84548, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>NEIGHBORHOOD</th>\n",
       "      <th>BUILDING CLASS CATEGORY</th>\n",
       "      <th>TAX CLASS AT PRESENT</th>\n",
       "      <th>BLOCK</th>\n",
       "      <th>BUILDING CLASS AT PRESENT</th>\n",
       "      <th>APARTMENT NUMBER</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>RESIDENTIAL UNITS</th>\n",
       "      <th>COMMERCIAL UNITS</th>\n",
       "      <th>TOTAL UNITS</th>\n",
       "      <th>LAND SQUARE FEET</th>\n",
       "      <th>GROSS SQUARE FEET</th>\n",
       "      <th>YEAR BUILT</th>\n",
       "      <th>TAX CLASS AT TIME OF SALE</th>\n",
       "      <th>BUILDING CLASS AT TIME OF SALE</th>\n",
       "      <th>SALE PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ALPHABET CITY</td>\n",
       "      <td>07 RENTALS - WALKUP APARTMENTS</td>\n",
       "      <td>2A</td>\n",
       "      <td>392</td>\n",
       "      <td>C2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10009</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1633</td>\n",
       "      <td>6440</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>C2</td>\n",
       "      <td>6625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ALPHABET CITY</td>\n",
       "      <td>07 RENTALS - WALKUP APARTMENTS</td>\n",
       "      <td>2</td>\n",
       "      <td>399</td>\n",
       "      <td>C7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10009</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>4616</td>\n",
       "      <td>18690</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>C7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ALPHABET CITY</td>\n",
       "      <td>07 RENTALS - WALKUP APARTMENTS</td>\n",
       "      <td>2</td>\n",
       "      <td>399</td>\n",
       "      <td>C7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10009</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2212</td>\n",
       "      <td>7803</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>C7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ALPHABET CITY</td>\n",
       "      <td>07 RENTALS - WALKUP APARTMENTS</td>\n",
       "      <td>2B</td>\n",
       "      <td>402</td>\n",
       "      <td>C4</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10009</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2272</td>\n",
       "      <td>6794</td>\n",
       "      <td>1913</td>\n",
       "      <td>2</td>\n",
       "      <td>C4</td>\n",
       "      <td>3936272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ALPHABET CITY</td>\n",
       "      <td>07 RENTALS - WALKUP APARTMENTS</td>\n",
       "      <td>2A</td>\n",
       "      <td>404</td>\n",
       "      <td>C2</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>10009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2369</td>\n",
       "      <td>4615</td>\n",
       "      <td>1900</td>\n",
       "      <td>2</td>\n",
       "      <td>C2</td>\n",
       "      <td>8000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BOROUGH   NEIGHBORHOOD                      BUILDING CLASS CATEGORY  \\\n",
       "0        1  ALPHABET CITY  07 RENTALS - WALKUP APARTMENTS                \n",
       "1        1  ALPHABET CITY  07 RENTALS - WALKUP APARTMENTS                \n",
       "2        1  ALPHABET CITY  07 RENTALS - WALKUP APARTMENTS                \n",
       "3        1  ALPHABET CITY  07 RENTALS - WALKUP APARTMENTS                \n",
       "4        1  ALPHABET CITY  07 RENTALS - WALKUP APARTMENTS                \n",
       "\n",
       "  TAX CLASS AT PRESENT  BLOCK BUILDING CLASS AT PRESENT APARTMENT NUMBER  \\\n",
       "0                   2A    392                        C2             <NA>   \n",
       "1                    2    399                        C7             <NA>   \n",
       "2                    2    399                        C7             <NA>   \n",
       "3                   2B    402                        C4             <NA>   \n",
       "4                   2A    404                        C2             <NA>   \n",
       "\n",
       "   ZIP CODE  RESIDENTIAL UNITS  COMMERCIAL UNITS  TOTAL UNITS  \\\n",
       "0     10009                  5                 0            5   \n",
       "1     10009                 28                 3           31   \n",
       "2     10009                 16                 1           17   \n",
       "3     10009                 10                 0           10   \n",
       "4     10009                  6                 0            6   \n",
       "\n",
       "  LAND SQUARE FEET GROSS SQUARE FEET  YEAR BUILT  TAX CLASS AT TIME OF SALE  \\\n",
       "0             1633              6440        1900                          2   \n",
       "1             4616             18690        1900                          2   \n",
       "2             2212              7803        1900                          2   \n",
       "3             2272              6794        1913                          2   \n",
       "4             2369              4615        1900                          2   \n",
       "\n",
       "  BUILDING CLASS AT TIME OF SALE SALE PRICE  \n",
       "0                             C2    6625000  \n",
       "1                             C7       <NA>  \n",
       "2                             C7       <NA>  \n",
       "3                             C4    3936272  \n",
       "4                             C2    8000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataset statistics\n",
    "\n",
    "print('(Rows, Columns):', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with total units != commercial units + residential units: 0\n",
      "\n",
      "\n",
      "Data type of features BOROUGH                            int64\n",
      "NEIGHBORHOOD                      object\n",
      "BUILDING CLASS CATEGORY           object\n",
      "TAX CLASS AT PRESENT              object\n",
      "BLOCK                              int64\n",
      "BUILDING CLASS AT PRESENT         object\n",
      "APARTMENT NUMBER                  object\n",
      "ZIP CODE                           int64\n",
      "RESIDENTIAL UNITS                  int64\n",
      "COMMERCIAL UNITS                   int64\n",
      "TOTAL UNITS                        int64\n",
      "LAND SQUARE FEET                  object\n",
      "GROSS SQUARE FEET                 object\n",
      "YEAR BUILT                         int64\n",
      "TAX CLASS AT TIME OF SALE          int64\n",
      "BUILDING CLASS AT TIME OF SALE    object\n",
      "SALE PRICE                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with total units is not equal to commercial units + residential units\n",
    "df = df[df[\"TOTAL UNITS\"] == (df[\"COMMERCIAL UNITS\"] + df[\"RESIDENTIAL UNITS\"])]\n",
    "\n",
    "print(\n",
    "    \"Rows with total units != commercial units + residential units:\",\n",
    "    df[df[\"TOTAL UNITS\"] != df[\"COMMERCIAL UNITS\"] + df[\"RESIDENTIAL UNITS\"]].shape[0],\n",
    ")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check data type of features\n",
    "print(\"Data type of features\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns:\n",
      "['BOROUGH', 'NEIGHBORHOOD', 'BUILDING CLASS CATEGORY', 'TAX CLASS AT PRESENT', 'BUILDING CLASS AT PRESENT', 'APARTMENT NUMBER', 'LAND SQUARE FEET', 'GROSS SQUARE FEET', 'TAX CLASS AT TIME OF SALE', 'BUILDING CLASS AT TIME OF SALE', 'SALE PRICE'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find categorical columns - columns with less than 10 unique values considered cateogrical for the purpose\n",
    "\n",
    "categorical_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object' or df[column].nunique() < 10:\n",
    "        categorical_columns.append(column)\n",
    "\n",
    "\n",
    "print('Categorical columns:')\n",
    "print(categorical_columns, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out categorical features from 'categorical_columns' if they are truly useful in categorical sense\n",
    "categorical_columns = [\n",
    "    'BOROUGH',\n",
    "    'NEIGHBORHOOD',\n",
    "    'TAX CLASS AT PRESENT',\n",
    "    'BUILDING CLASS CATEGORY',\n",
    "    'BUILDING CLASS AT PRESENT',\n",
    "    'TAX CLASS AT TIME OF SALE',\n",
    "    'BUILDING CLASS AT TIME OF SALE',\n",
    "]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    # Convert to categorical data type\n",
    "    df[col] = df[col].astype('category')\n",
    "    # Do label encoding\n",
    "    df[col] = df[col].cat.codes\n",
    "\n",
    "# Convert other feature data types to numeric wherever suitable\n",
    "df['SALE PRICE'] = pd.to_numeric(df['SALE PRICE'])\n",
    "df['LAND SQUARE FEET'] = pd.to_numeric(df['LAND SQUARE FEET'], errors='coerce')\n",
    "df['GROSS SQUARE FEET']= pd.to_numeric(df['GROSS SQUARE FEET'], errors='coerce')\n",
    "\n",
    "# # Convert SALE DATE to datetime and extact year\n",
    "# df['SALE DATE'] = pd.to_datetime(df['SALE DATE'], errors='coerce')\n",
    "# # df['SALE YEAR'] = df['SALE DATE'].dt.year\n",
    "# df[\"SALE DATE\"] = pd.DatetimeIndex(df[\"SALE DATE\"]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of features:\n"
     ]
    }
   ],
   "source": [
    "# Check if all features are appropriately set as per thier data types\n",
    "print(\"Data type of features:\")\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histogram for numerical data\n",
    "# for column in df.columns:\n",
    "#     # Check if the column is numeric\n",
    "#     if pd.api.types.is_numeric_dtype(df[column]):\n",
    "#         # Plot a histogram for numeric data\n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         sns.histplot(df[column], kde=True)\n",
    "#         plt.title(f\"Histogram of {column}\")\n",
    "#         plt.xlabel(column)\n",
    "#         plt.ylabel(\"Frequency\")\n",
    "#         plt.show()\n",
    "\n",
    "    # # Check if the column is categorical\n",
    "    # elif pd.api.types.is_categorical_dtype(df[column]):\n",
    "    #     # Plot a countplot for categorical data\n",
    "    #     plt.figure(figsize=(8, 4))\n",
    "    #     sns.countplot(x=column, data=df)\n",
    "    #     plt.title(f'Countplot of {column}')\n",
    "    #     plt.xlabel(column)\n",
    "    #     plt.ylabel('Count')\n",
    "    #     plt.xticks(rotation=45)\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Missing data - understand percentage of missing data\n",
    "\n",
    "\n",
    "def show_missing_values(dataframe):\n",
    "    # Calculate the percentage of missing data in each column\n",
    "    missing_percentage = dataframe.isnull().mean() * 100\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_percentage.plot(kind=\"bar\")\n",
    "    plt.ylabel(\"Percentage of Missing Data\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.title(\"Percentage of Missing Data by Feature\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "show_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "\n",
    "# get nunique values of zip code\n",
    "df['ZIP CODE'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treating Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null or 0 values after cleanup from SALE PRICE: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58960 entries, 0 to 84547\n",
      "Data columns (total 16 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   BOROUGH                         58960 non-null  int8   \n",
      " 1   NEIGHBORHOOD                    58960 non-null  int16  \n",
      " 2   BUILDING CLASS CATEGORY         58960 non-null  int8   \n",
      " 3   TAX CLASS AT PRESENT            58960 non-null  int8   \n",
      " 4   BLOCK                           58960 non-null  int64  \n",
      " 5   BUILDING CLASS AT PRESENT       58960 non-null  int16  \n",
      " 6   ZIP CODE                        58960 non-null  int64  \n",
      " 7   RESIDENTIAL UNITS               58960 non-null  int64  \n",
      " 8   COMMERCIAL UNITS                58960 non-null  int64  \n",
      " 9   TOTAL UNITS                     58960 non-null  int64  \n",
      " 10  LAND SQUARE FEET                38270 non-null  float64\n",
      " 11  GROSS SQUARE FEET               37719 non-null  float64\n",
      " 12  YEAR BUILT                      58960 non-null  int64  \n",
      " 13  TAX CLASS AT TIME OF SALE       58960 non-null  int8   \n",
      " 14  BUILDING CLASS AT TIME OF SALE  58960 non-null  int16  \n",
      " 15  SALE PRICE                      58960 non-null  float64\n",
      "dtypes: float64(3), int16(3), int64(6), int8(4)\n",
      "memory usage: 5.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0        742\n",
       "450000.0    426\n",
       "550000.0    414\n",
       "650000.0    413\n",
       "600000.0    407\n",
       "           ... \n",
       "313627.0      1\n",
       "347295.0      1\n",
       "371500.0      1\n",
       "458784.0      1\n",
       "69300.0       1\n",
       "Name: SALE PRICE, Length: 9742, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treating missing values\n",
    "\n",
    "# Cateogorical values for SALE PRICE\n",
    "SALE_PRICE_LABELS = [\"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "\n",
    "# Remove rows with missing or 0 values in SALE PRICE which is target variable\n",
    "df[\"SALE PRICE\"] = df[\"SALE PRICE\"].apply(lambda x: np.NAN if x <= 0 or \"\" else x)\n",
    "df.dropna(subset=[\"SALE PRICE\"], inplace=True)\n",
    "\n",
    "# Check if SALE PRICE has any NA values\n",
    "print(\n",
    "    \"Number of null or 0 values after cleanup from SALE PRICE:\",\n",
    "    df[\"SALE PRICE\"].isna().sum(),\n",
    ")\n",
    "\n",
    "\n",
    "# Delete the APARTMENT NUMBER since 77% of the values are missing and it is not a useful feature\n",
    "df.drop(\"APARTMENT NUMBER\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Remove rows with missing values in TAX CLASS AT PRESENT and BUILDING CLASS AT PRESENT\n",
    "df.dropna(subset=[\"TAX CLASS AT PRESENT\", \"BUILDING CLASS AT PRESENT\"], inplace=True)\n",
    "\n",
    "# Do k-means clustering to remove outliers from all numeric features\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def kmeans_remove_outliers(df, n_clusters=5, random_state=0):\n",
    "    # Standardize selected features\n",
    "    scaler = StandardScaler()\n",
    "    standardized_features = scaler.fit_transform(df)\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(standardized_features)\n",
    "    # Calculate distances of each data point from the cluster centers\n",
    "    distances = kmeans.transform(standardized_features)\n",
    "    # Find closest cluster for each data point\n",
    "    closest_cluster_distances = np.min(distances, axis=1)\n",
    "    # Determine the threshold value for outliers\n",
    "    threshold_distance = np.mean(closest_cluster_distances) + 3 * np.std(closest_cluster_distances)\n",
    "    # Flag the outliers\n",
    "    outliers = closest_cluster_distances > threshold_distance\n",
    "    # Remove the outliers\n",
    "    df = df[~outliers]\n",
    "    # Change SALE PRICE to categorical variable\n",
    "    df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n",
    "    return df\n",
    "\n",
    "\n",
    "df[\"SALE PRICE\"].describe()\n",
    "\n",
    "# change SALE PRICE to categorical variable\n",
    "# SALE_PRICE_LABELS = [\"Low\", \"Medium\", \"High\", \"Very High\"]\n",
    "# df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if LAND SQUARE FEET and GROSS SQUARE FEET are normally distributed\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# sns.histplot(df['LAND SQUARE FEET'], kde=True)\n",
    "# plt.title('Histogram of LAND SQUARE FEET')\n",
    "# plt.xlabel('LAND SQUARE FEET')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# sns.histplot(df['GROSS SQUARE FEET'], kde=True)\n",
    "# plt.title('Histogram of GROSS SQUARE FEET')\n",
    "# plt.xlabel('GROSS SQUARE FEET')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/jnqxrkbx5_x2fyjcbdhqfr3h0000gn/T/ipykernel_91896/3642752013.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n",
      "/var/folders/dv/jnqxrkbx5_x2fyjcbdhqfr3h0000gn/T/ipykernel_91896/3642752013.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n",
      "/var/folders/dv/jnqxrkbx5_x2fyjcbdhqfr3h0000gn/T/ipykernel_91896/3642752013.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n",
      "/var/folders/dv/jnqxrkbx5_x2fyjcbdhqfr3h0000gn/T/ipykernel_91896/3642752013.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"SALE PRICE\"] = pd.qcut(df[\"SALE PRICE\"], q=4, labels=SALE_PRICE_LABELS)\n"
     ]
    }
   ],
   "source": [
    "# Create duplicate df for imputation\n",
    "df_median_impute = df.copy()\n",
    "df_mean_inpute = df.copy()\n",
    "df_knn_impute = df.copy()\n",
    "df_no_impute = df.copy()\n",
    "\n",
    "# Impute the missing values in LAND SQUARE FEET and GROSS SQUARE FEET using different methods\n",
    "\n",
    "# Impute using median\n",
    "df_median_impute['LAND SQUARE FEET'] = df_median_impute['LAND SQUARE FEET'].fillna(df_median_impute['LAND SQUARE FEET'].median())\n",
    "df_median_impute['GROSS SQUARE FEET'] = df_median_impute['GROSS SQUARE FEET'].fillna(df_median_impute['GROSS SQUARE FEET'].median())\n",
    "# # do log1p transformation to make the data more normally distributed\n",
    "# df_median_impute['LAND SQUARE FEET'] = np.log1p(df_median_impute['LAND SQUARE FEET'])\n",
    "# df_median_impute['GROSS SQUARE FEET'] = np.log1p(df_median_impute['GROSS SQUARE FEET'])\n",
    "\n",
    "df_median_impute = kmeans_remove_outliers(df_median_impute)\n",
    "\n",
    "# Impute using mean\n",
    "df_mean_inpute['LAND SQUARE FEET'] = df_mean_inpute['LAND SQUARE FEET'].fillna(df_mean_inpute['LAND SQUARE FEET'].mean())\n",
    "df_mean_inpute['GROSS SQUARE FEET'] = df_mean_inpute['GROSS SQUARE FEET'].fillna(df_mean_inpute['GROSS SQUARE FEET'].mean())\n",
    "# # do log1p transformation to make the data more normally distributed\n",
    "# df_mean_inpute['LAND SQUARE FEET'] = np.log1p(df_mean_inpute['LAND SQUARE FEET'])\n",
    "# df_mean_inpute['GROSS SQUARE FEET'] = np.log1p(df_mean_inpute['GROSS SQUARE FEET'])\n",
    "df_mean_inpute = kmeans_remove_outliers(df_mean_inpute)\n",
    "\n",
    "# Impute using KNN\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_impute['LAND SQUARE FEET'] = imputer.fit_transform(df_knn_impute[['LAND SQUARE FEET']])\n",
    "df_knn_impute['GROSS SQUARE FEET'] = imputer.fit_transform(df_knn_impute[['GROSS SQUARE FEET']])\n",
    "# # do log1p transformation to make the data more normally distributed\n",
    "# df_knn_impute['LAND SQUARE FEET'] = np.log1p(df_knn_impute['LAND SQUARE FEET'])\n",
    "# df_knn_impute['GROSS SQUARE FEET'] = np.log1p(df_knn_impute['GROSS SQUARE FEET'])\n",
    "df_knn_impute = kmeans_remove_outliers(df_knn_impute)\n",
    "\n",
    "# Delete rows with missing values fir df_no_impute\n",
    "df_no_impute.dropna(inplace=True)\n",
    "# # do log1p transformation to make the data more normally distributed\n",
    "# df_no_impute['LAND SQUARE FEET'] = np.log1p(df_no_impute['LAND SQUARE FEET'])\n",
    "# df_no_impute['GROSS SQUARE FEET'] = np.log1p(df_no_impute['GROSS SQUARE FEET'])\n",
    "df_no_impute = kmeans_remove_outliers(df_no_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing missing values after cleanup\n",
    "# show_missing_values(df_median_impute)\n",
    "# show_missing_values(df_mean_inpute)\n",
    "# show_missing_values(df_knn_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    print(title)\n",
    "    print(cm)\n",
    "    # ax = plt.subplot()\n",
    "    # sns.heatmap(cm, annot=True, fmt=\"g\", ax=ax)\n",
    "    # # annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "    # # labels, title and ticks\n",
    "    # ax.set_xlabel(\"Predicted labels\")\n",
    "    # ax.set_ylabel(\"True labels\")\n",
    "    # ax.set_title(title)\n",
    "    # ax.xaxis.set_ticklabels(SALE_PRICE_LABELS)\n",
    "    # ax.yaxis.set_ticklabels(SALE_PRICE_LABELS)\n",
    "\n",
    "\n",
    "def get_predictions(model, X_train, X_test, y_train, y_test):\n",
    "    # Fit the model to the training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# Return optimal k\n",
    "def get_optimal_k(dataframe):\n",
    "    # Get the feature and target columns\n",
    "    X = dataframe.drop(\"SALE PRICE\", axis=1)\n",
    "    y = dataframe[\"SALE PRICE\"]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    k_values = range(1, 21)\n",
    "    accuracy_map = dict()\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        target_pred = get_predictions(knn, X_train, X_test, y_train, y_test)\n",
    "        accuracy_map[k] = accuracy_score(y_test, target_pred) * 100\n",
    "\n",
    "    # Plot the accuracy for different values of k\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, accuracy_map.values())\n",
    "    plt.xticks(k_values)\n",
    "    plt.xlabel(\"Value of k\")\n",
    "    plt.ylabel(\"Testing Accuracy\")\n",
    "    plt.title(\"Accuracy for different values of k\")\n",
    "    plt.show()\n",
    "\n",
    "    # Get optimal k\n",
    "    optimal_k = max(accuracy_map, key=accuracy_map.get)\n",
    "    return optimal_k\n",
    "\n",
    "# Return model based on type passed\n",
    "def get_model(model_type, dataframe):\n",
    "    if model_type == \"gaussian_nb\":\n",
    "        return GaussianNB()\n",
    "    elif model_type == \"decision_tree\":\n",
    "        return DecisionTreeClassifier()\n",
    "    elif model_type == \"cart_5\":\n",
    "        return DecisionTreeClassifier(max_depth=5)\n",
    "    elif model_type == \"random_forest\":\n",
    "        return RandomForestClassifier()\n",
    "    elif model_type == \"svm\":\n",
    "        return SVC()\n",
    "    elif model_type == \"logistic_regression\":\n",
    "        return LogisticRegression()\n",
    "    elif model_type == \"knn\":\n",
    "        # Note for testing - Uncomment below code to get optimal k. It has been commented to avoid running it everytime.\n",
    "        # optimal_k = get_optimal_k(dataframe)\n",
    "        # Above function returns optimal k = 8\n",
    "        optimal_k = 8\n",
    "        print(\"The optimal number of neighbors is {}\".format(optimal_k))\n",
    "        return KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "    # elif model_type == \"xg_boost\":\n",
    "    #     return xgb.XGBClassifier()\n",
    "    # elif model_type == \"sequential_dense\":\n",
    "    #     return Sequential()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Write a common function to train and test the different models\n",
    "def train_and_test_model(model_type, dataframe):\n",
    "    # Get the model\n",
    "    model = get_model(model_type, dataframe)\n",
    "\n",
    "    # Get the feature and target columns\n",
    "    X = dataframe.drop(\"SALE PRICE\", axis=1)\n",
    "    y = dataframe[\"SALE PRICE\"]\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    target_pred = get_predictions(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Get the confusion matrix\n",
    "    cm = confusion_matrix(y_test, target_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plot_confusion_matrix(cm, \"Confusion Matrix for \" + model_type)\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(y_test, target_pred))\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, target_pred))\n",
    "\n",
    "def use_xg_boost_model(dataframe):\n",
    "    # Initialize the label encoder for the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Copy the dataframe to avoid modifying the original data\n",
    "    df = dataframe.copy()\n",
    "\n",
    "    # Fit and transform the 'SALE PRICE' column\n",
    "    df[\"SALE PRICE\"] = label_encoder.fit_transform(df[\"SALE PRICE\"])\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=[\"category\"]).columns\n",
    "\n",
    "    # Use pd.get_dummies() to create dummy variables for categorical columns\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Split the data into features and target\n",
    "    X = df.drop(\"SALE PRICE\", axis=1)\n",
    "    y = df[\"SALE PRICE\"]\n",
    "\n",
    "    # Splitting the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create a XGBClassifier model\n",
    "    xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "    # Fit the model to the training set\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "    # Generate and print the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(cm, \"Confusion Matrix for XGBClassifier\")\n",
    "\n",
    "    # Print the classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "def use_sequential_dense_modal(dataframe):\n",
    "\n",
    "    # Get the feature and target columns\n",
    "    X = dataframe.drop(\"SALE PRICE\", axis=1)\n",
    "    y = dataframe[\"SALE PRICE\"]\n",
    "\n",
    "    # Convert the target column to categorical (one-hot encoding)\n",
    "    y_encoded = to_categorical(y.cat.codes)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Get the number of input features\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation=\"relu\", input_shape=(n_features,)))\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"softmax\"))  # Output layer for 4-class classification\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, y_train, epochs=10, validation_split=0.3)\n",
    "\n",
    "    # Evaluating the Model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:  Median Impute\n",
      "\n",
      "\n",
      "KNN\n",
      "The optimal number of neighbors is 8\n",
      "Confusion Matrix for knn\n",
      "[[2682  233  670  790]\n",
      " [ 438 2877  826  222]\n",
      " [ 991  949 2150  222]\n",
      " [ 981   71  124 3219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.61      0.57      4375\n",
      "         Low       0.70      0.66      0.68      4363\n",
      "      Medium       0.57      0.50      0.53      4312\n",
      "   Very High       0.72      0.73      0.73      4395\n",
      "\n",
      "    accuracy                           0.63     17445\n",
      "   macro avg       0.63      0.63      0.63     17445\n",
      "weighted avg       0.63      0.63      0.63     17445\n",
      "\n",
      "Accuracy: 0.6264259100028662\n",
      "\n",
      "\n",
      "Gaussian NB\n",
      "Confusion Matrix for gaussian_nb\n",
      "[[1268  192 2776  139]\n",
      " [ 624  354 3250  135]\n",
      " [ 776  171 3292   73]\n",
      " [2076  233 1401  685]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.27      0.29      0.28      4375\n",
      "         Low       0.37      0.08      0.13      4363\n",
      "      Medium       0.31      0.76      0.44      4312\n",
      "   Very High       0.66      0.16      0.25      4395\n",
      "\n",
      "    accuracy                           0.32     17445\n",
      "   macro avg       0.40      0.32      0.28     17445\n",
      "weighted avg       0.40      0.32      0.27     17445\n",
      "\n",
      "Accuracy: 0.32095156205216396\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix for decision_tree\n",
      "[[2494  379  792  710]\n",
      " [ 377 2940  881  165]\n",
      " [ 922 1047 2169  174]\n",
      " [ 900  206  188 3101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.57      0.55      4375\n",
      "         Low       0.64      0.67      0.66      4363\n",
      "      Medium       0.54      0.50      0.52      4312\n",
      "   Very High       0.75      0.71      0.73      4395\n",
      "\n",
      "    accuracy                           0.61     17445\n",
      "   macro avg       0.61      0.61      0.61     17445\n",
      "weighted avg       0.62      0.61      0.61     17445\n",
      "\n",
      "Accuracy: 0.613585554600172\n",
      "\n",
      "\n",
      "CART 5\n",
      "Confusion Matrix for cart_5\n",
      "[[2005  106  802 1462]\n",
      " [ 567 2219 1209  368]\n",
      " [1119  519 2094  580]\n",
      " [ 807   28   84 3476]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.45      0.46      0.45      4375\n",
      "         Low       0.77      0.51      0.61      4363\n",
      "      Medium       0.50      0.49      0.49      4312\n",
      "   Very High       0.59      0.79      0.68      4395\n",
      "\n",
      "    accuracy                           0.56     17445\n",
      "   macro avg       0.58      0.56      0.56     17445\n",
      "weighted avg       0.58      0.56      0.56     17445\n",
      "\n",
      "Accuracy: 0.5614216107767268\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix for random_forest\n",
      "[[2687  172  751  765]\n",
      " [ 333 2891  951  188]\n",
      " [ 789  776 2553  194]\n",
      " [ 749   73  122 3451]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.59      0.61      0.60      4375\n",
      "         Low       0.74      0.66      0.70      4363\n",
      "      Medium       0.58      0.59      0.59      4312\n",
      "   Very High       0.75      0.79      0.77      4395\n",
      "\n",
      "    accuracy                           0.66     17445\n",
      "   macro avg       0.67      0.66      0.66     17445\n",
      "weighted avg       0.67      0.66      0.66     17445\n",
      "\n",
      "Accuracy: 0.6639151619375179\n",
      "\n",
      "\n",
      "SVM\n",
      "Confusion Matrix for svm\n",
      "[[1887  413  467 1608]\n",
      " [1223 1194 1230  716]\n",
      " [1004  474 1732 1102]\n",
      " [ 860   51   14 3470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.43      0.40      4375\n",
      "         Low       0.56      0.27      0.37      4363\n",
      "      Medium       0.50      0.40      0.45      4312\n",
      "   Very High       0.50      0.79      0.61      4395\n",
      "\n",
      "    accuracy                           0.47     17445\n",
      "   macro avg       0.49      0.47      0.46     17445\n",
      "weighted avg       0.49      0.47      0.46     17445\n",
      "\n",
      "Accuracy: 0.47480653482373175\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudhchintha/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for logistic_regression\n",
      "[[ 922 1125  686 1642]\n",
      " [ 749 2074  679  861]\n",
      " [ 751 1378 1094 1089]\n",
      " [ 437  557  186 3215]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.32      0.21      0.25      4375\n",
      "         Low       0.40      0.48      0.44      4363\n",
      "      Medium       0.41      0.25      0.31      4312\n",
      "   Very High       0.47      0.73      0.57      4395\n",
      "\n",
      "    accuracy                           0.42     17445\n",
      "   macro avg       0.40      0.42      0.40     17445\n",
      "weighted avg       0.40      0.42      0.40     17445\n",
      "\n",
      "Accuracy: 0.41874462596732587\n",
      "\n",
      "\n",
      "XG Boost\n",
      "Confusion Matrix for XGBClassifier\n",
      "[[2657  127  699  892]\n",
      " [ 317 2858  945  243]\n",
      " [ 798  696 2545  273]\n",
      " [ 710   22   73 3590]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60      4375\n",
      "           1       0.77      0.66      0.71      4363\n",
      "           2       0.60      0.59      0.59      4312\n",
      "           3       0.72      0.82      0.76      4395\n",
      "\n",
      "    accuracy                           0.67     17445\n",
      "   macro avg       0.67      0.67      0.67     17445\n",
      "weighted avg       0.67      0.67      0.67     17445\n",
      "\n",
      "Accuracy: 0.6678131269704787\n",
      "\n",
      "\n",
      "Sequential Dense Model\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 909us/step - loss: 1.0577 - accuracy: 0.5371 - val_loss: 0.9877 - val_accuracy: 0.5604\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 1s 835us/step - loss: 0.9797 - accuracy: 0.5692 - val_loss: 0.9567 - val_accuracy: 0.5787\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 1s 793us/step - loss: 0.9561 - accuracy: 0.5812 - val_loss: 0.9503 - val_accuracy: 0.5911\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 1s 740us/step - loss: 0.9404 - accuracy: 0.5897 - val_loss: 0.9322 - val_accuracy: 0.5958\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 1s 733us/step - loss: 0.9278 - accuracy: 0.5962 - val_loss: 0.9231 - val_accuracy: 0.5983\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 1s 733us/step - loss: 0.9200 - accuracy: 0.6001 - val_loss: 0.9323 - val_accuracy: 0.5978\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 1s 727us/step - loss: 0.9157 - accuracy: 0.6021 - val_loss: 0.9269 - val_accuracy: 0.5997\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 1s 748us/step - loss: 0.9072 - accuracy: 0.6060 - val_loss: 0.9076 - val_accuracy: 0.6063\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 1s 782us/step - loss: 0.9038 - accuracy: 0.6093 - val_loss: 0.9098 - val_accuracy: 0.6047\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 1s 739us/step - loss: 0.8987 - accuracy: 0.6118 - val_loss: 0.8999 - val_accuracy: 0.6105\n",
      "546/546 [==============================] - 0s 381us/step - loss: 0.9164 - accuracy: 0.6159\n",
      "Accuracy: 0.6159358024597168\n",
      "\n",
      "\n",
      "\n",
      "Dataframe:  Mean Impute\n",
      "\n",
      "\n",
      "KNN\n",
      "The optimal number of neighbors is 8\n",
      "Confusion Matrix for knn\n",
      "[[2678  237  670  790]\n",
      " [ 445 2872  823  223]\n",
      " [ 984  921 2180  227]\n",
      " [ 973   89  120 3213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.61      0.57      4375\n",
      "         Low       0.70      0.66      0.68      4363\n",
      "      Medium       0.57      0.51      0.54      4312\n",
      "   Very High       0.72      0.73      0.73      4395\n",
      "\n",
      "    accuracy                           0.63     17445\n",
      "   macro avg       0.63      0.63      0.63     17445\n",
      "weighted avg       0.63      0.63      0.63     17445\n",
      "\n",
      "Accuracy: 0.6272857552307252\n",
      "\n",
      "\n",
      "Gaussian NB\n",
      "Confusion Matrix for gaussian_nb\n",
      "[[1168  207 2788  212]\n",
      " [ 520  393 3232  218]\n",
      " [ 734  202 3272  104]\n",
      " [1795  223 1496  881]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.28      0.27      0.27      4375\n",
      "         Low       0.38      0.09      0.15      4363\n",
      "      Medium       0.30      0.76      0.43      4312\n",
      "   Very High       0.62      0.20      0.30      4395\n",
      "\n",
      "    accuracy                           0.33     17445\n",
      "   macro avg       0.40      0.33      0.29     17445\n",
      "weighted avg       0.40      0.33      0.29     17445\n",
      "\n",
      "Accuracy: 0.32754370879908284\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix for decision_tree\n",
      "[[2518  354  789  714]\n",
      " [ 377 2946  870  170]\n",
      " [ 899 1034 2190  189]\n",
      " [ 948  201  196 3050]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.58      0.55      4375\n",
      "         Low       0.65      0.68      0.66      4363\n",
      "      Medium       0.54      0.51      0.52      4312\n",
      "   Very High       0.74      0.69      0.72      4395\n",
      "\n",
      "    accuracy                           0.61     17445\n",
      "   macro avg       0.62      0.61      0.61     17445\n",
      "weighted avg       0.62      0.61      0.61     17445\n",
      "\n",
      "Accuracy: 0.613585554600172\n",
      "\n",
      "\n",
      "CART 5\n",
      "Confusion Matrix for cart_5\n",
      "[[2015  108 1056 1196]\n",
      " [ 535 2221 1320  287]\n",
      " [1007  521 2277  507]\n",
      " [1101   29  131 3134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.43      0.46      0.45      4375\n",
      "         Low       0.77      0.51      0.61      4363\n",
      "      Medium       0.48      0.53      0.50      4312\n",
      "   Very High       0.61      0.71      0.66      4395\n",
      "\n",
      "    accuracy                           0.55     17445\n",
      "   macro avg       0.57      0.55      0.55     17445\n",
      "weighted avg       0.57      0.55      0.55     17445\n",
      "\n",
      "Accuracy: 0.5529951275437088\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix for random_forest\n",
      "[[2669  176  766  764]\n",
      " [ 326 2891  959  187]\n",
      " [ 761  777 2573  201]\n",
      " [ 758   76  113 3448]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.59      0.61      0.60      4375\n",
      "         Low       0.74      0.66      0.70      4363\n",
      "      Medium       0.58      0.60      0.59      4312\n",
      "   Very High       0.75      0.78      0.77      4395\n",
      "\n",
      "    accuracy                           0.66     17445\n",
      "   macro avg       0.67      0.66      0.66     17445\n",
      "weighted avg       0.67      0.66      0.66     17445\n",
      "\n",
      "Accuracy: 0.6638578389223273\n",
      "\n",
      "\n",
      "SVM\n",
      "Confusion Matrix for svm\n",
      "[[1650  526  679 1520]\n",
      " [ 466 2126 1146  625]\n",
      " [ 590  725 2052  945]\n",
      " [ 694  310   36 3355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.49      0.38      0.42      4375\n",
      "         Low       0.58      0.49      0.53      4363\n",
      "      Medium       0.52      0.48      0.50      4312\n",
      "   Very High       0.52      0.76      0.62      4395\n",
      "\n",
      "    accuracy                           0.53     17445\n",
      "   macro avg       0.53      0.53      0.52     17445\n",
      "weighted avg       0.53      0.53      0.52     17445\n",
      "\n",
      "Accuracy: 0.5263972484952708\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudhchintha/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for logistic_regression\n",
      "[[ 917  917 1025 1516]\n",
      " [ 625 1510 1192 1036]\n",
      " [ 658 1013 1535 1106]\n",
      " [ 465  591  305 3034]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.34      0.21      0.26      4375\n",
      "         Low       0.37      0.35      0.36      4363\n",
      "      Medium       0.38      0.36      0.37      4312\n",
      "   Very High       0.45      0.69      0.55      4395\n",
      "\n",
      "    accuracy                           0.40     17445\n",
      "   macro avg       0.39      0.40      0.38     17445\n",
      "weighted avg       0.39      0.40      0.38     17445\n",
      "\n",
      "Accuracy: 0.40103181427343076\n",
      "\n",
      "\n",
      "XG Boost\n",
      "Confusion Matrix for XGBClassifier\n",
      "[[2671  107  707  890]\n",
      " [ 358 2817  951  237]\n",
      " [ 802  661 2575  274]\n",
      " [ 710   34   72 3579]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60      4375\n",
      "           1       0.78      0.65      0.71      4363\n",
      "           2       0.60      0.60      0.60      4312\n",
      "           3       0.72      0.81      0.76      4395\n",
      "\n",
      "    accuracy                           0.67     17445\n",
      "   macro avg       0.67      0.67      0.67     17445\n",
      "weighted avg       0.67      0.67      0.67     17445\n",
      "\n",
      "Accuracy: 0.6673545428489539\n",
      "\n",
      "\n",
      "Sequential Dense Model\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 837us/step - loss: 1.0562 - accuracy: 0.5330 - val_loss: 0.9884 - val_accuracy: 0.5698\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 1s 740us/step - loss: 0.9824 - accuracy: 0.5681 - val_loss: 0.9612 - val_accuracy: 0.5788\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 1s 742us/step - loss: 0.9598 - accuracy: 0.5801 - val_loss: 0.9507 - val_accuracy: 0.5862\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 1s 745us/step - loss: 0.9462 - accuracy: 0.5862 - val_loss: 0.9404 - val_accuracy: 0.5901\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 1s 750us/step - loss: 0.9340 - accuracy: 0.5917 - val_loss: 0.9309 - val_accuracy: 0.5911\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 1s 741us/step - loss: 0.9268 - accuracy: 0.5974 - val_loss: 0.9221 - val_accuracy: 0.6009\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 1s 748us/step - loss: 0.9203 - accuracy: 0.5976 - val_loss: 0.9155 - val_accuracy: 0.6078\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 1s 736us/step - loss: 0.9135 - accuracy: 0.5999 - val_loss: 0.9224 - val_accuracy: 0.5979\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 1s 732us/step - loss: 0.9068 - accuracy: 0.6060 - val_loss: 0.9053 - val_accuracy: 0.6114\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 1s 737us/step - loss: 0.9032 - accuracy: 0.6094 - val_loss: 0.9023 - val_accuracy: 0.6086\n",
      "546/546 [==============================] - 0s 374us/step - loss: 0.9076 - accuracy: 0.6146\n",
      "Accuracy: 0.614560067653656\n",
      "\n",
      "\n",
      "\n",
      "Dataframe:  KNN Impute\n",
      "\n",
      "\n",
      "KNN\n",
      "The optimal number of neighbors is 8\n",
      "Confusion Matrix for knn\n",
      "[[2678  237  670  790]\n",
      " [ 445 2872  823  223]\n",
      " [ 984  921 2180  227]\n",
      " [ 973   89  120 3213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.61      0.57      4375\n",
      "         Low       0.70      0.66      0.68      4363\n",
      "      Medium       0.57      0.51      0.54      4312\n",
      "   Very High       0.72      0.73      0.73      4395\n",
      "\n",
      "    accuracy                           0.63     17445\n",
      "   macro avg       0.63      0.63      0.63     17445\n",
      "weighted avg       0.63      0.63      0.63     17445\n",
      "\n",
      "Accuracy: 0.6272857552307252\n",
      "\n",
      "\n",
      "Gaussian NB\n",
      "Confusion Matrix for gaussian_nb\n",
      "[[1168  207 2788  212]\n",
      " [ 520  393 3232  218]\n",
      " [ 734  202 3272  104]\n",
      " [1795  223 1496  881]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.28      0.27      0.27      4375\n",
      "         Low       0.38      0.09      0.15      4363\n",
      "      Medium       0.30      0.76      0.43      4312\n",
      "   Very High       0.62      0.20      0.30      4395\n",
      "\n",
      "    accuracy                           0.33     17445\n",
      "   macro avg       0.40      0.33      0.29     17445\n",
      "weighted avg       0.40      0.33      0.29     17445\n",
      "\n",
      "Accuracy: 0.32754370879908284\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix for decision_tree\n",
      "[[2507  350  812  706]\n",
      " [ 376 2942  865  180]\n",
      " [ 911 1034 2187  180]\n",
      " [ 922  211  190 3072]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.53      0.57      0.55      4375\n",
      "         Low       0.65      0.67      0.66      4363\n",
      "      Medium       0.54      0.51      0.52      4312\n",
      "   Very High       0.74      0.70      0.72      4395\n",
      "\n",
      "    accuracy                           0.61     17445\n",
      "   macro avg       0.62      0.61      0.61     17445\n",
      "weighted avg       0.62      0.61      0.61     17445\n",
      "\n",
      "Accuracy: 0.6138148466609343\n",
      "\n",
      "\n",
      "CART 5\n",
      "Confusion Matrix for cart_5\n",
      "[[2015  108 1056 1196]\n",
      " [ 535 2221 1320  287]\n",
      " [1007  521 2277  507]\n",
      " [1101   29  131 3134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.43      0.46      0.45      4375\n",
      "         Low       0.77      0.51      0.61      4363\n",
      "      Medium       0.48      0.53      0.50      4312\n",
      "   Very High       0.61      0.71      0.66      4395\n",
      "\n",
      "    accuracy                           0.55     17445\n",
      "   macro avg       0.57      0.55      0.55     17445\n",
      "weighted avg       0.57      0.55      0.55     17445\n",
      "\n",
      "Accuracy: 0.5529951275437088\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix for random_forest\n",
      "[[2677  169  759  770]\n",
      " [ 338 2892  948  185]\n",
      " [ 792  760 2557  203]\n",
      " [ 747   76  116 3456]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.59      0.61      0.60      4375\n",
      "         Low       0.74      0.66      0.70      4363\n",
      "      Medium       0.58      0.59      0.59      4312\n",
      "   Very High       0.75      0.79      0.77      4395\n",
      "\n",
      "    accuracy                           0.66     17445\n",
      "   macro avg       0.67      0.66      0.66     17445\n",
      "weighted avg       0.67      0.66      0.66     17445\n",
      "\n",
      "Accuracy: 0.6639151619375179\n",
      "\n",
      "\n",
      "SVM\n",
      "Confusion Matrix for svm\n",
      "[[1650  526  679 1520]\n",
      " [ 466 2126 1146  625]\n",
      " [ 590  725 2052  945]\n",
      " [ 694  310   36 3355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.49      0.38      0.42      4375\n",
      "         Low       0.58      0.49      0.53      4363\n",
      "      Medium       0.52      0.48      0.50      4312\n",
      "   Very High       0.52      0.76      0.62      4395\n",
      "\n",
      "    accuracy                           0.53     17445\n",
      "   macro avg       0.53      0.53      0.52     17445\n",
      "weighted avg       0.53      0.53      0.52     17445\n",
      "\n",
      "Accuracy: 0.5263972484952708\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudhchintha/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for logistic_regression\n",
      "[[ 917  917 1025 1516]\n",
      " [ 625 1510 1192 1036]\n",
      " [ 658 1013 1535 1106]\n",
      " [ 465  591  305 3034]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.34      0.21      0.26      4375\n",
      "         Low       0.37      0.35      0.36      4363\n",
      "      Medium       0.38      0.36      0.37      4312\n",
      "   Very High       0.45      0.69      0.55      4395\n",
      "\n",
      "    accuracy                           0.40     17445\n",
      "   macro avg       0.39      0.40      0.38     17445\n",
      "weighted avg       0.39      0.40      0.38     17445\n",
      "\n",
      "Accuracy: 0.40103181427343076\n",
      "\n",
      "\n",
      "XG Boost\n",
      "Confusion Matrix for XGBClassifier\n",
      "[[2671  107  707  890]\n",
      " [ 358 2817  951  237]\n",
      " [ 802  661 2575  274]\n",
      " [ 710   34   72 3579]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60      4375\n",
      "           1       0.78      0.65      0.71      4363\n",
      "           2       0.60      0.60      0.60      4312\n",
      "           3       0.72      0.81      0.76      4395\n",
      "\n",
      "    accuracy                           0.67     17445\n",
      "   macro avg       0.67      0.67      0.67     17445\n",
      "weighted avg       0.67      0.67      0.67     17445\n",
      "\n",
      "Accuracy: 0.6673545428489539\n",
      "\n",
      "\n",
      "Sequential Dense Model\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 1s 881us/step - loss: 1.0567 - accuracy: 0.5333 - val_loss: 0.9888 - val_accuracy: 0.5564\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 1s 811us/step - loss: 0.9765 - accuracy: 0.5687 - val_loss: 0.9623 - val_accuracy: 0.5767\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 1s 828us/step - loss: 0.9539 - accuracy: 0.5844 - val_loss: 0.9472 - val_accuracy: 0.5866\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 1s 814us/step - loss: 0.9404 - accuracy: 0.5907 - val_loss: 0.9357 - val_accuracy: 0.5916\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 1s 766us/step - loss: 0.9312 - accuracy: 0.5947 - val_loss: 0.9288 - val_accuracy: 0.5952\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 1s 847us/step - loss: 0.9224 - accuracy: 0.5968 - val_loss: 0.9275 - val_accuracy: 0.6010\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 1s 727us/step - loss: 0.9174 - accuracy: 0.5990 - val_loss: 0.9227 - val_accuracy: 0.5980\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 1s 722us/step - loss: 0.9117 - accuracy: 0.6022 - val_loss: 0.9102 - val_accuracy: 0.6090\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 1s 726us/step - loss: 0.9079 - accuracy: 0.6075 - val_loss: 0.9092 - val_accuracy: 0.6077\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 1s 760us/step - loss: 0.9026 - accuracy: 0.6077 - val_loss: 0.9067 - val_accuracy: 0.6072\n",
      "546/546 [==============================] - 0s 381us/step - loss: 0.9224 - accuracy: 0.6113\n",
      "Accuracy: 0.6113499402999878\n",
      "\n",
      "\n",
      "\n",
      "Dataframe:  No Impute\n",
      "\n",
      "\n",
      "KNN\n",
      "The optimal number of neighbors is 8\n",
      "Confusion Matrix for knn\n",
      "[[1742  215  412  446]\n",
      " [ 365 1651  628  202]\n",
      " [ 677  714 1250  128]\n",
      " [ 559   57   51 2097]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.52      0.62      0.57      2815\n",
      "         Low       0.63      0.58      0.60      2846\n",
      "      Medium       0.53      0.45      0.49      2769\n",
      "   Very High       0.73      0.76      0.74      2764\n",
      "\n",
      "    accuracy                           0.60     11194\n",
      "   macro avg       0.60      0.60      0.60     11194\n",
      "weighted avg       0.60      0.60      0.60     11194\n",
      "\n",
      "Accuracy: 0.6021082722887261\n",
      "\n",
      "\n",
      "Gaussian NB\n",
      "Confusion Matrix for gaussian_nb\n",
      "[[ 565  121 2058   71]\n",
      " [ 572  171 1971  132]\n",
      " [ 379   66 2267   57]\n",
      " [ 854  229 1027  654]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.24      0.20      0.22      2815\n",
      "         Low       0.29      0.06      0.10      2846\n",
      "      Medium       0.31      0.82      0.45      2769\n",
      "   Very High       0.72      0.24      0.36      2764\n",
      "\n",
      "    accuracy                           0.33     11194\n",
      "   macro avg       0.39      0.33      0.28     11194\n",
      "weighted avg       0.39      0.33      0.28     11194\n",
      "\n",
      "Accuracy: 0.32669287118098983\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix for decision_tree\n",
      "[[1519  325  531  440]\n",
      " [ 335 1659  698  154]\n",
      " [ 587  781 1297  104]\n",
      " [ 506  190  131 1937]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.52      0.54      0.53      2815\n",
      "         Low       0.56      0.58      0.57      2846\n",
      "      Medium       0.49      0.47      0.48      2769\n",
      "   Very High       0.74      0.70      0.72      2764\n",
      "\n",
      "    accuracy                           0.57     11194\n",
      "   macro avg       0.58      0.57      0.57     11194\n",
      "weighted avg       0.57      0.57      0.57     11194\n",
      "\n",
      "Accuracy: 0.5728068608182955\n",
      "\n",
      "\n",
      "CART 5\n",
      "Confusion Matrix for cart_5\n",
      "[[1817   71  596  331]\n",
      " [ 538 1054 1054  200]\n",
      " [ 785  208 1665  111]\n",
      " [ 962   19   56 1727]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.44      0.65      0.53      2815\n",
      "         Low       0.78      0.37      0.50      2846\n",
      "      Medium       0.49      0.60      0.54      2769\n",
      "   Very High       0.73      0.62      0.67      2764\n",
      "\n",
      "    accuracy                           0.56     11194\n",
      "   macro avg       0.61      0.56      0.56     11194\n",
      "weighted avg       0.61      0.56      0.56     11194\n",
      "\n",
      "Accuracy: 0.5594961586564231\n",
      "\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix for random_forest\n",
      "[[1738  168  468  441]\n",
      " [ 301 1650  725  170]\n",
      " [ 522  606 1530  111]\n",
      " [ 384   53   60 2267]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.59      0.62      0.60      2815\n",
      "         Low       0.67      0.58      0.62      2846\n",
      "      Medium       0.55      0.55      0.55      2769\n",
      "   Very High       0.76      0.82      0.79      2764\n",
      "\n",
      "    accuracy                           0.64     11194\n",
      "   macro avg       0.64      0.64      0.64     11194\n",
      "weighted avg       0.64      0.64      0.64     11194\n",
      "\n",
      "Accuracy: 0.6418617116312311\n",
      "\n",
      "\n",
      "SVM\n",
      "Confusion Matrix for svm\n",
      "[[1292  168  667  688]\n",
      " [ 392  966 1192  296]\n",
      " [ 533  241 1723  272]\n",
      " [ 710   48   84 1922]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.44      0.46      0.45      2815\n",
      "         Low       0.68      0.34      0.45      2846\n",
      "      Medium       0.47      0.62      0.54      2769\n",
      "   Very High       0.60      0.70      0.65      2764\n",
      "\n",
      "    accuracy                           0.53     11194\n",
      "   macro avg       0.55      0.53      0.52     11194\n",
      "weighted avg       0.55      0.53      0.52     11194\n",
      "\n",
      "Accuracy: 0.5273360728961944\n",
      "\n",
      "\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anirudhchintha/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for logistic_regression\n",
      "[[ 792  587  582  854]\n",
      " [ 439  897  936  574]\n",
      " [ 457  690 1085  537]\n",
      " [ 496  347  149 1772]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.36      0.28      0.32      2815\n",
      "         Low       0.36      0.32      0.33      2846\n",
      "      Medium       0.39      0.39      0.39      2769\n",
      "   Very High       0.47      0.64      0.55      2764\n",
      "\n",
      "    accuracy                           0.41     11194\n",
      "   macro avg       0.40      0.41      0.40     11194\n",
      "weighted avg       0.40      0.41      0.40     11194\n",
      "\n",
      "Accuracy: 0.40611041629444344\n",
      "\n",
      "\n",
      "XG Boost\n",
      "Confusion Matrix for XGBClassifier\n",
      "[[1810  129  432  444]\n",
      " [ 317 1619  725  185]\n",
      " [ 550  485 1624  110]\n",
      " [ 371   32   31 2330]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.62      2815\n",
      "           1       0.71      0.57      0.63      2846\n",
      "           2       0.58      0.59      0.58      2769\n",
      "           3       0.76      0.84      0.80      2764\n",
      "\n",
      "    accuracy                           0.66     11194\n",
      "   macro avg       0.66      0.66      0.66     11194\n",
      "weighted avg       0.66      0.66      0.66     11194\n",
      "\n",
      "Accuracy: 0.6595497587993568\n",
      "\n",
      "\n",
      "Sequential Dense Model\n",
      "Epoch 1/10\n",
      "572/572 [==============================] - 1s 834us/step - loss: 1.0920 - accuracy: 0.5090 - val_loss: 1.0472 - val_accuracy: 0.5300\n",
      "Epoch 2/10\n",
      "572/572 [==============================] - 0s 762us/step - loss: 1.0034 - accuracy: 0.5603 - val_loss: 1.0015 - val_accuracy: 0.5664\n",
      "Epoch 3/10\n",
      "572/572 [==============================] - 0s 850us/step - loss: 0.9742 - accuracy: 0.5747 - val_loss: 0.9798 - val_accuracy: 0.5758\n",
      "Epoch 4/10\n",
      "572/572 [==============================] - 0s 774us/step - loss: 0.9553 - accuracy: 0.5794 - val_loss: 0.9623 - val_accuracy: 0.5823\n",
      "Epoch 5/10\n",
      "572/572 [==============================] - 0s 754us/step - loss: 0.9410 - accuracy: 0.5918 - val_loss: 0.9753 - val_accuracy: 0.5749\n",
      "Epoch 6/10\n",
      "572/572 [==============================] - 0s 792us/step - loss: 0.9314 - accuracy: 0.5976 - val_loss: 0.9526 - val_accuracy: 0.5881\n",
      "Epoch 7/10\n",
      "572/572 [==============================] - 0s 865us/step - loss: 0.9198 - accuracy: 0.5994 - val_loss: 0.9402 - val_accuracy: 0.5937\n",
      "Epoch 8/10\n",
      "572/572 [==============================] - 0s 831us/step - loss: 0.9148 - accuracy: 0.6052 - val_loss: 0.9425 - val_accuracy: 0.5807\n",
      "Epoch 9/10\n",
      "572/572 [==============================] - 0s 787us/step - loss: 0.9094 - accuracy: 0.6072 - val_loss: 0.9335 - val_accuracy: 0.5966\n",
      "Epoch 10/10\n",
      "572/572 [==============================] - 0s 765us/step - loss: 0.9040 - accuracy: 0.6113 - val_loss: 0.9308 - val_accuracy: 0.5997\n",
      "350/350 [==============================] - 0s 382us/step - loss: 0.9147 - accuracy: 0.6104\n",
      "Accuracy: 0.6104162931442261\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_map = {\n",
    "    \"Median Impute\": df_median_impute,\n",
    "    \"Mean Impute\": df_mean_inpute,\n",
    "    \"KNN Impute\": df_knn_impute,\n",
    "    \"No Impute\": df_no_impute,\n",
    "}\n",
    "\n",
    "for df_key in df_map.keys():\n",
    "    # Get df_name from key\n",
    "    print(\"Dataframe: \", df_key)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    df_item = df_map[df_key]\n",
    "\n",
    "    print(\"KNN\")\n",
    "    train_and_test_model(\"knn\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Gaussian NB\")\n",
    "    train_and_test_model(\"gaussian_nb\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Decision Tree\")\n",
    "    train_and_test_model(\"decision_tree\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"CART 5\")\n",
    "    train_and_test_model(\"cart_5\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Random Forest\")\n",
    "    train_and_test_model(\"random_forest\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"SVM\")\n",
    "    train_and_test_model(\"svm\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    print(\"Logistic Regression\")\n",
    "    train_and_test_model(\"logistic_regression\", df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"XG Boost\")\n",
    "    use_xg_boost_model(df_item)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Sequential Dense Model\")\n",
    "    use_sequential_dense_modal(df_item)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
